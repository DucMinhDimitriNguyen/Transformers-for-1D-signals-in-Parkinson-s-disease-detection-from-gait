{"cells":[{"cell_type":"markdown","metadata":{"id":"3ea99e5e"},"source":["Parkinson detection"],"id":"3ea99e5e"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1645536987947,"user":{"displayName":"Serge Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIsf8tpj8SUvq-Tvdf9MrB8oxBk6kwWurlI9V1aA=s64","userId":"05009995910480739700"},"user_tz":-60},"id":"ODITyDNFRzg9","outputId":"eaba8582-f882-4d37-cd32-50a513316445"},"outputs":[{"output_type":"stream","name":"stdout","text":["              total        used        free      shared  buff/cache   available\n","Mem:            25G        622M         22G        1.2M        2.1G         24G\n","Swap:            0B          0B          0B\n"]}],"source":["!free -h\n","!df -h #disk\n","!cat /proc/cpuinfo #cpu\n","!cat /proc/meminfo #memory\n","!nvidia-smi -L #GPU count and name\n"],"id":"ODITyDNFRzg9"},{"cell_type":"markdown","metadata":{"id":"b9646f29"},"source":["Algo"],"id":"b9646f29"},{"cell_type":"code","execution_count":null,"metadata":{"id":"24d186da"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","@author: Nguyen Duc Minh\n","\"\"\"\n","\n","import numpy as np\n","np.random.seed(2)\n","import tensorflow as tf\n","tf.config.run_functions_eagerly(True)\n","#tf.enable_eager_execution()\n","from tensorflow.keras import layers\n","from tensorflow.keras import Input\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dropout, Flatten, Conv1D\n","\n","from tensorflow.keras.layers import Dense, Flatten, Dropout\n","from tensorflow.keras import backend as K\n","import uuid\n","\n","def add_pos_2(input,nb):\n","    input_pos_encoding = tf.constant(nb, shape=[input.shape[1]], dtype=\"int32\")/input.shape[1]\n","    input_pos_encoding = tf.cast(tf.reshape(input_pos_encoding, [1,10]),tf.float32)\n","    input = tf.add(input ,input_pos_encoding)\n","    return input\n","\n","def stack_block_transformer(num_transformer_blocks):\n","    input1 = keras.Input(shape=(100, 1))\n","    x = input1\n","    for _ in range(num_transformer_blocks):\n","        x = transformer_encoder(x,100,2)\n","    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n","    x = layers.Dense(10, activation='selu')(x)\n","    return input1,x\n","\n","def stack_block_transformer_spatial(num_transformer_blocks,x):\n","  for _ in range(num_transformer_blocks):\n","      x = transformer_encoder(x,10*18,2)\n","  x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n","\n","  return x\n","\n","def transformer_encoder(inputs,key_dim,num_heads):\n","    dropout=0.1\n","    # Normalization and Attention\n","    print(\"transformer_encoder\",inputs.shape)\n","    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n","    x = layers.MultiHeadAttention(\n","        key_dim=key_dim, num_heads=num_heads\n","    )(x, x)\n","    x = layers.Dropout(dropout)(x)\n","    res = x + inputs\n","\n","    # Feed Forward Part\n","    x = layers.LayerNormalization(epsilon=1e-6)(res)\n","    x = layers.Dense(key_dim, activation='softmax')(x)\n","    return x + res\n","\n","\n","def multiple_transformer(nb):\n","    '''\n","\n","    :param nb: number of features ( indicates the number of parallel branches)\n","    :return:\n","    '''\n","    # initialise with the first input\n","\n","    num_transformer_blocks = 2  #hyperparameter\n","    input_, transformer_ = stack_block_transformer(num_transformer_blocks)\n","    transformers = []\n","    inputs = []\n","    transformers.append(transformer_)\n","    inputs.append(input_)\n","    for i in range(1,nb ):\n","        input_i, transformer_i = stack_block_transformer(num_transformer_blocks)\n","        inputs.append(input_i) \n","        transformer_i = add_pos_2(transformer_i,i)\n","        transformers.append(transformer_i)\n","  \n","    x = layers.concatenate(transformers, axis=-1)\n","    x = tf.expand_dims(x, -1) #-1 denotes the last dimension\n","    x = stack_block_transformer_spatial(num_transformer_blocks,x)\n","    x = Dropout(0.1)(x)\n","    x = layers.Dense(100, activation='selu')(x)\n","    x = Dropout(0.1)(x)\n","    x = layers.Dense(20, activation='selu')(x)\n","    x = Dropout(0.1)(x)\n","    answer = layers.Dense(1, activation='sigmoid')(x)\n","  \n","    model = Model(inputs, answer)\n","    opt = optimizers.RMSprop(lr=0.001)\n","    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'],experimental_run_tf_function=False)\n","    print(model.summary())\n","    return model\n","\n","\n","def multiple_transformer_5_level(nb):\n","    '''\n","    Model for severity prediction , 5 classes output\n","    :param nb:  number of parallel branch\n","    :return:\n","    '''\n","\n","  # initialise with the first input\n","\n","    num_transformer_blocks = 2  #hyperparameter\n","    input_, transformer_ = stack_block_transformer(num_transformer_blocks)\n","    transformers = []\n","    inputs = []\n","    transformers.append(transformer_)\n","    inputs.append(input_)\n","    for i in range(1,nb ):\n","        input_i, transformer_i = stack_block_transformer(num_transformer_blocks)\n","        inputs.append(input_i)\n","        transformer_i = add_pos_2(transformer_i,i)\n","        transformers.append(transformer_i)\n","  \n","    x = layers.concatenate(transformers, axis=-1)\n","    x = tf.expand_dims(x, -1) #-1 denotes the last dimension\n","    x = stack_block_transformer_spatial(num_transformer_blocks,x)\n","    x = Dropout(0.1)(x)\n","    x = layers.Dense(100, activation='selu')(x)\n","    x = Dropout(0.1)(x)\n","    x = layers.Dense(20, activation='selu')(x)\n","    x = Dropout(0.1)(x)\n","    answer = layers.Dense(5, activation='sigmoid')(x)\n","  \n","    model = Model(inputs, answer)\n","    opt = optimizers.RMSprop(lr=0.001)\n","    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'],experimental_run_tf_function=False)\n","    print(model.summary())\n","    return model\n","\n","\n"],"id":"24d186da"},{"cell_type":"markdown","metadata":{"id":"7359e3aa"},"source":["Data_utils"],"id":"7359e3aa"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ea853f10"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","\n","\"\"\"\n","import glob\n","import os\n","import random\n","import sys\n","from tensorflow import keras\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical\n","\n","sys.path.append('../src')\n","np.random.seed(2)\n","\n","\n","\n","class Data:\n","\n","    def __init__(self,  input_data,  deep, gait_cycle, step=50, features=np.arange(1, 19), pk_level = True):\n","        '''\n","\n","        :param load_or_get:  1: load data , 0: load preloaded datas ( npy)\n","        :param deep:  data in the format for deep learning algorithms\n","        :param gait_cycle: number of gait cycle per signal\n","        :param step: overlap between gait signals\n","        :param features: signals to be loaded ( coming from sensors)\n","        :param pk_level: if true , y is the parkinson level according\n","        '''\n","\n","        self.deep = deep\n","        self.step = step\n","        self.nb_gait_cycle = gait_cycle\n","\n","\n","        self.features_to_load = features\n","        self.nb_features = self.features_to_load.shape[0]\n","        ###############\n","        self.X_data = np.array([])  # np.ones((self.nb_gait_cycle,self.nb_features))\n","        self.y_data = np.array([])\n","        self.nb_data_per_person = np.array([0])\n","\n","\n","        files = sorted(glob.glob(os.path.join(input_data, '*txt')))\n","        self.ctrl_list = []\n","        self.pk_list = []\n","        for file in files:\n","\n","            if file.find(\".txt\") != -1:  # if control (\"01.txt\")\n","                if file.find(\"Co\") != -1:  # if control\n","                    self.ctrl_list.append(file)\n","                elif file.find(\"Pt\") != -1:  # if control\n","                    if pk_level:\n","                        if  file.find('SiPt02')!= -1:\n","                            pass\n","                        elif file.find('SiPt07')!= -1:\n","                            pass\n","                        else:\n","                            self.pk_list.append(file)\n","                    else :\n","                        self.pk_list.append(file)\n","\n","        random.shuffle(self.ctrl_list)\n","        random.shuffle(self.pk_list)\n","        self.pk_level = pk_level\n","        if pk_level == True:\n","            self.levels = pd.read_csv( os.path.join(input_data, \"demographics.csv\"))\n","            self.levels.set_index('ID', inplace=True)\n","        self.load(norm=None)\n","     \n","    def add_pos(self,input):\n","       # Positional encoding\n","        input_pos_encoding = tf.range(input.shape[1])/input.shape[1]\n","        input_pos_encoding = tf.expand_dims(input_pos_encoding, -1)\n","        input_pos_encoding= tf.cast(tf.tile(input_pos_encoding, [1,input.shape[2]]),tf.float32)\n","        # Add the positional encoding\n","        input = input + input_pos_encoding\n","        return input\n","\n","\n","    def separate_fold(self, fold_number, total_fold=10):\n","        '''\n","\n","        :param fold_number: Fold number\n","        :param total_fold: Total number of fols\n","        :return:\n","        '''\n","        proportion = 1 / total_fold  # .10 for 10 folds\n","        X = [self.X_ctrl, self.X_park]\n","        y = [self.y_ctrl, self.y_park]\n","        patients = [self.nb_data_per_person[:self.last_ctrl_patient], self.nb_data_per_person[self.last_ctrl_patient:]] # counts separated by classe\n","        patients[1]= patients[1] - patients[1][0]\n","        diff_count = np.diff(self.nb_data_per_person)\n","        diff_count = [diff_count[:self.last_ctrl_patient], diff_count[self.last_ctrl_patient:]]\n","        self.count_val = np.array([0])\n","        self.count_train = np.array([0])\n","        for i in range(len(X)):\n","            nbr_patients =  int(len(patients[i]) *proportion)\n","            start_patient = int(fold_number*nbr_patients )\n","            end_patient = (fold_number+1)*nbr_patients\n","            id_start = patients[i][start_patient]  # segment start\n","            id_end = patients[i][end_patient]  # end segment\n","            if i ==0 :\n","                self.X_val = X[i][id_start:id_end,:,:]\n","                self.X_train = np.delete(X[i], np.arange(id_start,id_end) , 0)\n","\n","                self.y_val = y[i][id_start:id_end]\n","                self.y_train = np.delete(y[i], np.arange(id_start,id_end) , 0)\n","\n","\n","                self.count_val = np.append(self.count_val, diff_count[i][start_patient: end_patient])\n","                self.count_train = np.append(self.count_train, np.delete(diff_count[i], np.arange(start_patient, end_patient)))\n","\n","\n","\n","            else:\n","                start_patient = start_patient #+ patients[0].shape[0]  # patients0.shape 0 is the number of patients in the first class\n","                end_patient =  end_patient# +patients[0].shape[0]\n","                self.X_val = np.vstack((self.X_val, X[i][id_start:id_end,:,:]))\n","                self.X_train = np.vstack((self.X_train, np.delete(X[i], np.arange(id_start,id_end) , 0) ))\n","\n","                self.y_val = np.vstack((self.y_val, y[i][id_start:id_end] ))\n","                self.y_train = np.vstack((self.y_train, np.delete(y[i], np.arange(id_start,id_end) , 0) ))\n","\n","                self.count_val = np.append( self.count_val , diff_count[i][start_patient: end_patient])\n","                self.count_train = np.append(self.count_train,np.delete(diff_count[i], np.arange(start_patient, end_patient)) )\n","\n","        self.count_val = np.cumsum(self.count_val)\n","        self.count_train = np.cumsum(self.count_train )\n","        self.X_val = layers.LayerNormalization(epsilon=1e-6)(self.X_val)\n","        self.X_train = layers.LayerNormalization(epsilon=1e-6)(self.X_train)\n","        self.X_val = self.add_pos(self.X_val)\n","        self.X_train = self.add_pos(self.X_train)\n","      \n","\n","    def load(self, norm = 'std'):\n","        print(\"load training control \")\n","        self.load_data(self.ctrl_list, 0)\n","        if self.deep == 1:\n","            self.last_ctrl= self.X_data.shape[2]\n","            self.last_ctrl_patient = len(self.nb_data_per_person)\n","        print(\"load training parkinson \")\n","\n","\n","        self.load_data(self.pk_list, 1)  # ncycle, nfeature, nombre de data\n","\n","\n","        ## all datas are loaded at this point, preprocessing now\n","        if self.deep == 1:\n","            self.X_data = self.X_data.transpose(2,0 , 1)  #0, 1\n","\n","            if norm == 'std ':\n","                self.normalize()\n","            elif norm == 'l2':\n","                self.X_data = self.normalize_l2(self.X_data)\n","\n","        if self.pk_level:\n","            self.one_hot_encoding()\n","\n","        if self.deep == 1:\n","            self.X_ctrl = self.X_data[:self.last_ctrl]\n","            self.y_ctrl =  self.y_data[:self.last_ctrl]\n","            self.X_park = self.X_data[self.last_ctrl:]\n","            self.y_park = self.y_data[self.last_ctrl:]\n","\n","        print(\"saving training \")\n","        np.save(\"Xdata\", self.X_data)\n","        np.save(\"ydata\", self.y_data)\n","        np.save('data_person',self.nb_data_per_person)\n","        np.save('ctrl_list', self.ctrl_list)\n","        np.save('pk_list', self.pk_list)\n","\n","    def normalize(self):\n","        '''\n","\n","        :return: Normalize to have a mean =  and std =1\n","        '''\n","        mean_train = np.mean(self.X_data,(0,1))\n","        std_train = np.std(self.X_data,(0,1))\n","        self.X_data= abs((self.X_data - mean_train) / std_train)\n","        #self.X_test= (self.X_test - mean_train) / std_train\n","    def one_hot_encoding(self):\n","        '''\n","\n","        :return: return one hot encoding vector for severity prediction\n","        '''\n","        self.y_data[self.y_data<=4]=0\n","        self.y_data[(4<self.y_data) & (self.y_data <15)]=1\n","        self.y_data[(15<= self.y_data) & (self.y_data<25)]=2\n","        self.y_data[(25<= self.y_data) & (self.y_data<35)] = 3\n","        self.y_data[35<= self.y_data] = 4\n","        np.set_printoptions(threshold=sys.maxsize)\n","        # self.y_data = np.squeeze(self.y_data)\n","        #self.y_data = self.y_data[~np.isnan(self.y_data)] # Line 2\n","        self.y_data = np.nan_to_num(self.y_data)\n","        print(self.y_data)\n","        print(self.y_data.size)\n","        self.y_data = to_categorical(self.y_data)\n","\n","    def normalize_l2(self, data):\n","        '''\n","\n","        :param data:  Function to perform L2 normalization\n","        :return:\n","        '''\n","        data = keras.backend.l2_normalize(data, axis=(1, 2))\n","        data = tf.keras.backend.get_value(data)\n","        return data\n","\n","\n","    def load_data(self, liste, y):\n","        '''\n","\n","        :param liste: list of patients filepaths\n","        :param y: 0 for control, 1 for parkinson\n","        :return:\n","        '''\n","\n","        for i in range(0, len(liste)):\n","            datas = np.loadtxt(liste[i])  # num cycle, n features\n","            datas = datas[:, self.features_to_load]\n","            print(datas.shape[0])\n","            if  self.pk_level :\n","                print(\"1\")\n","                y =self.find_level(liste[i])\n","\n","            if self.deep == 1:\n","                print(\"2\")\n","                X_data, y_data , self.nb_data_per_person = self.generate_datas(datas, y, self.nb_data_per_person)\n","              \n","            else:\n","                print(\"3\")\n","                X_data, y_data = self.generate_datas_ml(datas, y)\n","            if (self.X_data).size == 0:\n","                print(\"4\")\n","                self.X_data = X_data\n","                self.y_data = y_data\n","            else:\n","                print(\"5\")\n","                if self.deep == 1:\n","                    print(\"6\")\n","                    self.X_data = np.dstack((self.X_data, X_data))\n","                else:\n","                    print(\"7\")\n","                    self.X_data = np.vstack((self.X_data, X_data))  # shape nb data --- vector size\n","                self.y_data = np.vstack((self.y_data, y_data))\n","\n","            print(X_data.shape, self.X_data.shape,flush=True)\n","           \n","\n","\n","    def find_level(self,file):\n","        '''\n","\n","        :param file: Dataframe\n","        :return:\n","        '''\n","        start = '../data/'\n","        end = '_'\n","        id = (file.split(start))[1].split(end)[0]\n","        y = self.levels.loc[id,'UPDRS']\n","        return y\n","\n","\n","    def generate_datas(self, datas, y, data_list):\n","        '''\n","\n","        :param datas:  datas loaded for 1 patient\n","        :param y: label of the patient\n","        :param data_list: list containing the number of segments per patients\n","        :return:\n","        '''\n","        count = 0\n","        X_data = np.array([])\n","        y_data = np.array([])\n","        nb_datas = int(datas.shape[0] - self.nb_gait_cycle)\n","        for start in range(0, nb_datas, self.step):\n","            end = start + self.nb_gait_cycle\n","            data = datas[start:end, :]\n","            if X_data.size == 0:\n","                X_data = data\n","                y_data = y\n","            else:\n","                if (self.deep == 1):\n","                    X_data = np.dstack((X_data, data))\n","                else:\n","                    X_data = np.vstack((X_data, data))\n","                y_data = np.vstack((y_data, y))\n","            count = count + 1\n","        data_list = np.append(data_list, count+ data_list[-1])\n","        return X_data, y_data, data_list\n","\n","\n","    def get_datas(self):\n","        return self.X_data, self.y_data, self.X_test, self.y_test, self.X_val, self.y_val\n","\n"],"id":"ea853f10"},{"cell_type":"markdown","metadata":{"id":"248c0eb7"},"source":["results"],"id":"248c0eb7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"662c1ad3"},"outputs":[],"source":["import os\n","import numpy as np\n","from sklearn.metrics import confusion_matrix,  classification_report, accuracy_score\n","from scipy import stats\n","import pandas as pd\n","class Results:\n","    def __init__(self, filename_seg, filename_patient):\n","        '''\n","\n","        :param filename_seg:  Filename  (.csv) where to save results at the segment levels\n","        :param filename_patient: Filename  (.csv) where to save results at the patient levels\n","        '''\n","        self.results_patients = np.zeros(3)\n","        self.results_segments = np.zeros(3)\n","        self.filename_seg = filename_seg\n","        self.filename_patient = filename_patient\n","    def add_result( self,res, accuracy,  segments = True  ):\n","        '''\n","\n","        :param res: result of classification report (sklearn )\n","        :param accuracy:\n","        :param segments: 1 to add results at the segment level\n","        :return:\n","        '''\n","        if segments:\n","            specificity = res['0.0']['recall']\n","            sensitivy =  res['1.0']['recall']\n","        else:\n","            specificity = res['0']['recall']\n","            sensitivy =  res['1']['recall']\n","        all = np.array([specificity, sensitivy, accuracy])\n","\n","        if segments:\n","            self.results_segments = np.vstack((self.results_segments, all))\n","        else:\n","            self.results_patients = np.vstack((self.results_patients, all ))\n","\n","    def validate_patient(self, model, x_val, y_val, count):\n","        '''\n","\n","        :param model: trained model after 1 fold of cross validation\n","        :param x_val: x_Val for 1 forld of cross validation\n","        :param y_val: y_Val for 1 forld of cross validation\n","        :param count: vector containing the number of segments per patient\n","        :return:  save the results of the fold\n","\n","        '''\n","        ## per segments\n","        pred_seg = model.predict(np.split(x_val, x_val.shape[2], axis=2))\n","        res = classification_report(np.rint(y_val), np.rint(pred_seg), output_dict = True )\n","        acc = accuracy_score(np.rint(y_val), np.rint(pred_seg))\n","        self.add_result(res, acc,True  )\n","\n","        eval = []\n","        y = []\n","        pred = []\n","        for m in range(1, len(count)):\n","            i = count[m]\n","            j = count[m - 1]\n","            score = model.evaluate(np.split(x_val[j:i, :, :], x_val.shape[2], axis=2), y_val[j:i])\n","            eval.append(score)\n","            y.append(np.int(np.mean(y_val[j:i])))\n","            p = np.rint(model.predict(np.split(x_val[j:i, :, :], x_val.shape[2], axis=2)))\n","            pred.append(np.mean(p))\n","\n","        res = classification_report(y, np.rint(pred), output_dict = True )\n","        print(classification_report(y, np.rint(pred)))\n","\n","        acc = accuracy_score(np.rint(y), np.rint(pred))\n","        self.add_result(res, acc, False )\n","\n","        #np.savetxt(self.filename_patient, self.results_patients, delimiter=\",\")\n","        #np.savetxt(self.filename_seg, self.results_segments, delimiter=\",\")\n","        res_segments_dict = {'Specificity': self.results_segments[1:,0],'Sensitivity': self.results_segments[1:,1],'Accuracy': self.results_segments[1:,2]  }\n","        df = pd.DataFrame.from_dict(res_segments_dict)\n","        df.to_csv(self.filename_seg)\n","        res_patients_dict =  {'Specificity': self.results_patients[1:,0],'Sensitivity': self.results_patients[1:,1],'Accuracy': self.results_patients[1:,2]  }\n","        df = pd.DataFrame.from_dict(res_patients_dict)\n","        df.to_csv(self.filename_patient)\n","\n","\n","\n","class Results_level:\n","    '''\n","    Class to save results for severity prediction\n","    '''\n","    def __init__(self, filename_seg, filename_patient, dir):\n","        '''\n","\n","        :param filename_seg: filename (csv) where to save the results\n","        :param filename_patient:\n","        :param dir: directory where results files are saved\n","        '''\n","        self.results_patients = np.zeros(1)\n","        self.results_segments = np.zeros(1)\n","        self.filename_seg = filename_seg\n","        self.filename_patient = filename_patient\n","        self.gt = np.array([])\n","        self.pred = np.array([])\n","        self.dir = dir\n","    def add_result( self,res, accuracy,  segments = True  ):\n","\n","        all = np.array([ accuracy])\n","\n","        if segments:\n","            self.results_segments = np.vstack((self.results_segments, all))\n","        else:\n","            self.results_patients = np.vstack((self.results_patients, all ))\n","\n","\n","\n","    def validate_patient(self, model, x_val, y_val, count):\n","        '''\n","\n","        :param model: trained model after 1 fold of cross validation\n","        :param x_val: x_Val for 1 forld of cross validation\n","        :param y_val: y_Val for 1 forld of cross validation\n","        :param count: vector containing the number of segments per patient\n","        :return:  save the results of the fold\n","\n","        '''\n","        ## per segments\n","        pred_seg = model.predict(np.split(x_val, x_val.shape[2], axis=2))\n","        res = classification_report(np.rint(y_val), np.rint(pred_seg), output_dict = True )\n","        acc = accuracy_score(np.rint(y_val), np.rint(pred_seg))\n","        self.add_result(res, acc,True  )\n","\n","        eval = []\n","        y = []\n","        pred = []\n","        for m in range(1, len(count)):\n","            i = count[m]\n","            j = count[m - 1]\n","            score = model.evaluate(np.split(x_val[j:i, :, :], x_val.shape[2], axis=2), y_val[j:i])\n","            eval.append(score)\n","            y_gt = np.argmax(y_val[j:i],1)\n","            y_gt , _ = stats.mode(y_gt, axis = None)\n","            y.append(y_gt[0])\n","            p = np.rint(model.predict(np.split(x_val[j:i, :, :], x_val.shape[2], axis=2)))\n","            p = np.argmax(p, 1 )\n","            p, _ = stats.mode(p, axis=None)\n","            pred.append(p[0])\n","\n","        res = classification_report(y, np.rint(pred), output_dict = True )\n","        print(classification_report(y, np.rint(pred)))\n","        self.gt = np.append(self.gt, y)\n","        self.pred = np.append(self.pred, np.rint(pred))\n","        acc = accuracy_score(np.rint(y), np.rint(pred))\n","        self.add_result(res, acc, False )\n","        res_segments_dict = {'Accuracy': self.results_segments[1:,0]}\n","        df = pd.DataFrame.from_dict(res_segments_dict)\n","        df.to_csv(self.filename_seg)\n","        res_patients_dict =  {'Accuracy': self.results_patients[1:,0]}\n","        df = pd.DataFrame.from_dict(res_patients_dict)\n","        df.to_csv(self.filename_patient)\n","        print(res)\n","\n","\n","\n","\n","    def write_results(self):\n","        '''\n","        Called at the end to write the final result files\n","        :return:\n","        '''\n","        res_segments_dict = {'Accuracy': self.results_segments[1:,0]}\n","        df = pd.DataFrame.from_dict(res_segments_dict)\n","        df.to_csv(self.filename_seg)\n","        res_patients_dict =  {'Accuracy': self.results_patients[1:,0]}\n","        df = pd.DataFrame.from_dict(res_patients_dict)\n","        df.to_csv(self.filename_patient)\n","        file_pred = os.path.join(self.dir, 'pred.csv')\n","        file_gt = os.path.join(self.dir, 'gt.csv')\n","        np.savetxt(file_pred, self.pred, delimiter=\",\" )\n","        np.savetxt(file_gt,self.gt, delimiter=\",\")\n","        res = classification_report(self.gt, self.pred)\n","        print(res)\n","        self.cm = confusion_matrix(self.gt, self.pred)\n","        file_conf_matrx = os.path.join(self.dir, 'confusion_matrix.csv')\n","        np.savetxt(file_conf_matrx, self.cm, delimiter=\",\")\n"],"id":"662c1ad3"},{"cell_type":"markdown","metadata":{"id":"c82fd0f8"},"source":["Train"],"id":"c82fd0f8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"31745805"},"outputs":[],"source":["import numpy as np\n","import argparse\n","# fix random seed for reproducibility\n","np.random.seed(2) #2\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n","import datetime\n","import os\n","\n","#from src.data_utils2 import Datas\n","#from src.results import Results,Results_level\n","\n","#from src.algo import multiple_cnn1D, multiple_cnn1D5_level\n","#from src.data_utils import Data\n","\n","def train( model, datas, lr, log_filename, filename):\n","    \"\"\"\n","\n","    :param model: Initial untrained model\n","    :param datas:  data object\n","    :param lr: learning rate\n","    :param log_filename: filename where the training results will be saved ( for each epoch)\n","    :param filename: file where the weights will be saved\n","    :return:  trained model\n","    \"\"\"\n","    X_train = datas.X_train\n","    y_train = datas.y_train\n","    X_val = datas.X_val\n","    y_val = datas.y_val\n","    \n","    \n","    logger = CSVLogger(log_filename, separator=',', append=True)\n","    for i in (np.arange(1,4)*5):  # 10-20    1-10\n","\n","        checkpointer = ModelCheckpoint(filepath=filename , monitor='val_accuracy', verbose=1, save_best_only=True,save_freq='epoch')\n","        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20, verbose=0, mode='auto')\n","\n","        callbacks_list = [checkpointer, early_stopping, logger]\n","        print(\"Y_train\")\n","        print(len(y_train))\n","        history = model.fit(np.split(X_train,X_train.shape[2], axis=2), \\\n","                            # history  = model.fit(X_data,\\\n","                            y_train, \\\n","                            verbose=1, \\\n","                            shuffle=True, \\\n","                            epochs= 100,\\\n","                            batch_size=110, \\\n","                            # validation_data=(X_val, y_val),\\\n","                            validation_data=(np.split(X_val, X_val.shape[2], axis=2), y_val), \\\n","                            callbacks=callbacks_list)\n","\n","        model.load_weights(filename)\n","        lr =  lr / 2\n","        rms = optimizers.Nadam(lr=lr)\n","        model.compile(loss='binary_crossentropy', optimizer=rms, metrics=['accuracy'])\n","        return model\n","\n","\n","def train_classifier(args):\n","    '''\n","    Function that performs the detection of Parkinson\n","    :param args: Input arguments\n","    :return:\n","    '''\n","    exp_name = args.exp_name\n","    subfolder = os.path.join(args.output, exp_name +'_' + datetime.datetime.now().strftime(\"%m_%d\"), datetime.datetime.now().strftime(\n","        \"%H_%M\"))\n","    file_result_patients = os.path.join(subfolder,'res_pat.csv')\n","    file_result_segments = os.path.join(subfolder,'res_seg.csv')\n","    model_file = os.path.join(subfolder, \"model.json\")\n","    if not os.path.exists(subfolder):\n","        os.makedirs(subfolder)\n","\n","    val_results = Results(file_result_segments, file_result_patients)\n","    datas = Data(args.input_data, 1, 100, pk_level= False )\n","\n","    for i in range(0, 10):\n","        lr = 0.001\n","        model = multiple_transformer(datas.X_data.shape[2])\n","        model_json = model.to_json()\n","        with open(model_file, \"w\") as json_file:\n","            json_file.write(model_json)\n","\n","        print('fold', str(i))\n","        datas.separate_fold(i)\n","        log_filename = os.path.join( subfolder ,\"training_\" + str(i) + \".csv\")\n","        w_filename = os.path.join(subfolder ,\"weights_\" + str(i) + \".hdf5\")\n","        model = train(model, datas, lr, log_filename, w_filename)\n","        print('Validation !!')\n","        val_results.validate_patient(model, datas.X_val, datas.y_val, datas.count_val)\n","\n","def train_severity(args):\n","    '''\n","\n","    :param args: Input arguments\n","    :return:\n","    '''\n","    features = np.arange(1, 19)\n","\n","\n","    exp_name = args.exp_name\n","\n","    subfolder = os.path.join(args.output, exp_name +'_' + datetime.datetime.now().strftime(\"%m_%d\"), datetime.datetime.now().strftime(\n","        \"%H_%M\"))\n","    if not os.path.exists(subfolder):\n","        os.makedirs(subfolder)\n","    file_result_patients = os.path.join(subfolder ,'res_pat.csv')\n","    file_result_segments = os.path.join(subfolder ,'res_seg.csv')\n","\n","    model_file = os.path.join(subfolder, \"model.json\")\n","\n","    val_results = Results_level(file_result_segments, file_result_patients, subfolder )\n","    datas = Data(args.input_data, 1, 100)  # modif\n","    lr = 0.001\n","    for i in range(0,10):\n","\n","        model = multiple_transformer_5_level(datas.X_data.shape[2])\n","        model_json = model.to_json()\n","        with open(model_file, \"w\") as json_file:\n","            json_file.write(model_json)\n","        print('fold', str(i))\n","        datas.separate_fold(i)\n","        log_filename = os.path.join(subfolder, \"trainig\" + str(i) + \".csv\")\n","        w_filename = os.path.join(subfolder ,\"weights_\" + str(i) + \".hdf5\")\n","        model = train(model, datas, lr, log_filename,  w_filename )\n","        print('Validation !!')\n","        val_results.validate_patient(model, datas.X_val, datas.y_val, datas.count_val)\n","         val_results.write_results()\n","\n","\n","if __name__ == '__main__':\n","    \n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"-input_data\", default='../data', type=str)\n","    #' \n","    parser.add_argument(\"-exp_name\", default='train_severity', type=str, help = 'train_classifier ; train_severity')\n","    parser.add_argument(\"-output\", default='output', type=str)\n","    args = parser.parse_args(args=[])\n","    if not os.path.exists(args.output):\n","        os.makedirs(args.output)\n","    if args.exp_name == 'train_classifier' :\n","        train_classifier(args)\n","    if args.exp_name == 'train_severity':\n","        train_severity(args)\n"],"id":"31745805"}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Copie de TransformatorParkinson V4.ipynb","provenance":[{"file_id":"10em1Cu4WsSJmZWTUOe_fJrdWAEhku5WI","timestamp":1636780860062},{"file_id":"1qxkcq304vVzhx97N4TqQA9NOJG7Bb_ft","timestamp":1635861057795},{"file_id":"1Ou3rbOWkof17i1_HqFAys9EjwJfMAwGw","timestamp":1631911337205},{"file_id":"https://github.com/minh28/Parkinson/blob/master/src/Parkinson.ipynb","timestamp":1631224609589}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}